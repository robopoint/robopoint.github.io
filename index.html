<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>RoboPoint</title>
    <meta name="description" content="RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics">
    <meta name="keywords" content="Foundation Model, Affordance Prediction">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script>
        function updateManipMethodVideo() {
            const method = document.getElementById("single-menu-method-manipulation").value;
            const video = document.getElementById("result-manipulation-video");
            const uri = "static/videos/results/manipulation/" + method + ".mp4"
            video.src = uri;
            video.playbackRate = 1.75;
            video.play();
        }
        function updateNavMethodVideo() {
            const method = document.getElementById("single-menu-method-navigation").value;
            const video = document.getElementById("result-navigation-video");
            const uri = "static/videos/results/navigation/" + method + ".mp4"
            video.src = uri;
            video.playbackRate = 1.75;
            video.play();
        }
    </script>
</head>

<body onload="updateManipMethodVideo(); updateNavMethodVideo();">
    <!-- Title -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics
                        </h1>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!--/ Title-->

    <!-- Teaser and Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Video Teaser -->
            <video id="teaser" autoplay muted loop height="100">
                <source src="static/videos/vid_intro_teaser.mp4" type="video/mp4">
            </video>
            <!-- /Video Teaser -->
            <br/>
            <br/>
            <!-- Abstract -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            From rearranging objects on a table to putting groceries into shelves,
                            robots must plan precise action points to perform tasks accurately and reliably. In
                            spite of the recent adoption of vision language models (VLMs) to control robot
                            behavior, VLMs struggle to precisely articulate robot actions using language. We
                            introduce an automatic synthetic data generation pipeline that instruction-tunes
                            VLMs to robotic domains and needs. Using the pipeline, we train <span class="drobopoint">ROBOPOINT</span>,
                            a VLM that predicts image keypoint affordances given language instructions. Compared
                            to alternative approaches, our method requires no real-world data collection
                            or human demonstration, making it much more scalable to diverse environments
                            and viewpoints. In addition, <span class="drobopoint">ROBOPOINT</span> is a general
                            model that enables several downstream applications such as robot navigation,
                            manipulation, and augmented reality (AR) assistance. Our experiments demonstrate
                            that <span class="drobopoint">ROBOPOINT</span> outperforms state-of-the-art
                            VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy
                            of predicting spatial affordance and by 30.5% in the success rate of downstream tasks.
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Abstract -->
        </div>
    </section>

    <section>
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Downstream Application</h2>
                    <div class="columns is-vcentered interpolation-panel">
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_ar.mp4" type="video/mp4">
                            </video>
                            <p >AR Assistance</p>
                        </div>
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_manipulation.mp4" type="video/mp4">
                            </video>
                            <p >Manipulation</p>
                        </div>
                        <div class="column  has-text-centered">
                            <video autoplay controls muted loop playsinline>
                                <source src="static/videos/vid_navigation.mp4" type="video/mp4">
                            </video>
                            <p >Navigation</p>
                        </div>
                        <div class="columns is-centered">
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">                        
                    <figure>
                        <img src="static/images/overview.png" alt="Robot Arm">
                        <figcaption>An RGB image is rendered from a procedurally generated 3D scene. We compute spatial relations from the camera's perspective and generate affordances by sampling points within object masks and object-surface intersections. These instruction-point pairs fine-tune the language model. During deployment, \method\ predicts 2D action points from an image and instruction, which are projected into 3D using a depth map. The robot then navigates to these 3D targets with a motion planner.</figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>

    <!-- Manipulation method comparison -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3">Results</h2>
                        <div class="columns">
                            <div class="column has-text-centered">
                                <h3 class="title is-5">Manipulation Results</h3>
                                Method
                                <div class="select is-small">
                                    <select id="single-menu-method-manipulation" onchange="updateManipMethodVideo()">
                                        <option value="gpt4v" selected="selected">GPT4-V</option>
                                        <option value="pivot">PIVOT</option>
                                        <option value="qwenvl">QwenVL</option>
                                        <option value="robopoint">RoboPoint</option>
                                    </select>
                                </div>
                                <br>
                                <br>
                                <video id="result-manipulation-video" muted autoplay loop>
                                    <source src="static/videos/results/manipulation/gpt4v.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- /Manipulation method comparison -->

    <!-- Navigation method comparison -->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <div class="columns">
                            <div class="column has-text-centered">
                                <h3 class="title is-5">Navigation Results</h3>
                                Method
                                <div class="select is-small">
                                    <select id="single-menu-method-navigation" onchange="updateNavMethodVideo()">
                                        <option value="gpt4v" selected="selected">GPT4-V</option>
                                        <option value="pivot">PIVOT</option>
                                        <option value="robopoint">RoboPoint</option>
                                    </select>
                                </div>
                                <br>
                                <br>
                                <video id="result-navigation-video" muted autoplay loop>
                                    <source src="static/videos/results/navigation/gpt4v.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- /Navigation method comparison -->


</body>
</html>
