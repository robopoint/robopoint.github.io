<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>RoboPoint</title>
    <meta name="description" content="RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics">
    <meta name="keywords" content="Foundation Model, Affordance Prediction">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
    <!-- Title -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics
                        </h1>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!--/ Title-->

    <!-- Teaser and Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Video Teaser -->
            <video id="teaser" autoplay muted loop height="100">
                <source src="static/videos/vid_intro_teaser.mp4" type="video/mp4">
            </video>
            <!-- /Video Teaser -->
            <br/>
            <br/>
            <!-- Abstract -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            From rearranging objects on a table to putting groceries into shelves,
                            robots must plan precise action points to perform tasks accurately and reliably. In
                            spite of the recent adoption of vision language models (VLMs) to control robot
                            behavior, VLMs struggle to precisely articulate robot actions using language. We
                            introduce an automatic synthetic data generation pipeline that instruction-tunes
                            VLMs to robotic domains and needs. Using the pipeline, we train <span class="drobopoint">ROBOPOINT</span>,
                            a VLM that predicts image keypoint affordances given language instructions. Compared
                            to alternative approaches, our method requires no real-world data collection
                            or human demonstration, making it much more scalable to diverse environments
                            and viewpoints. In addition, <span class="drobopoint">ROBOPOINT</span> is a general
                            model that enables several downstream applications such as robot navigation,
                            manipulation, and augmented reality (AR) assistance. Our experiments demonstrate
                            that <span class="drobopoint">ROBOPOINT</span> outperforms state-of-the-art
                            VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy
                            of predicting spatial affordance and by 30.5% in the success rate of downstream tasks.
                        </p>
                    </div>
                </div>
            </div>
            <!-- /Abstract -->
        </div>
    </section>

</body>
</html>
